å¥½
import os
import json
import requests
import sys
import threading
import logging
import subprocess
import time
from datetime import datetime
from flask import Flask, request, jsonify, make_response, send_file, Response, stream_with_context, send_from_directory
from flask_cors import CORS
import lunar_python
from lunar_python import Lunar, Solar
from master_book import MASTER_BOOK
from rule_engine import create_chart_from_dict, evaluate_rules, PALACE_NAMES

# --- Configuration & Constants Loading ---

def load_config():
    # Use absolute path relative to this script
    base_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(base_dir, 'config.json')
    
    defaults = {
        "server": {"host": "0.0.0.0", "port": 5000, "debug": False},
        # Use 127.0.0.1 by default
        "gemini": {"api_key": "", "model": "gemini-1.5-flash"},
        "app": {"title": "ç´«å¾®å…«å­— Â· å¤©æ©Ÿå‘½è­œç³»çµ±", "geometry": "1000x750", "icon_path": "icon.png"}
    }
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                for k, v in user_config.items():
                    if k in defaults and isinstance(v, dict):
                        defaults[k].update(v)
                    else:
                        defaults[k] = v
        except Exception as e:
            print(f"Error loading config.json: {e}")
    return defaults

def load_constants():
    # Use absolute path relative to this script
    base_dir = os.path.dirname(os.path.abspath(__file__))
    const_path = os.path.join(base_dir, 'ziwei_constants.json')
    defaults = {
        "STEMS": ["ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"],
        "BRANCHES": ["å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥"],
        "SI_HUA_TABLE": {}
    }
    if os.path.exists(const_path):
        try:
            with open(const_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                defaults.update(data)
        except Exception as e:
            print(f"Error loading ziwei_constants.json: {e}")
    return defaults

CONFIG = load_config()
CONSTANTS = load_constants()

# --- Backend Logic & Data Management ---

CHAT_LOG_FILE = 'chat_history.json'
RECORD_FILE = 'user_records.json'

def load_json_file(filename):
    if not os.path.exists(filename):
        return []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return []

def save_json_file(filename, data):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def log_chat(model, prompt, response):
    logs = load_json_file(CHAT_LOG_FILE)
    entry = {
        "timestamp": datetime.now().isoformat(),
        "model": model,
        "prompt": prompt,
        "response": response
    }
    logs.append(entry)
    if len(logs) > 1000:
        logs = logs[-1000:]
    save_json_file(CHAT_LOG_FILE, logs)

# --- App Globals ---
app = Flask(__name__)
# Enable CORS for all domains to allow proxy/remote access easily
CORS(app) 

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") or CONFIG['gemini'].get('api_key', "")
DEFAULT_MODEL = "gemini-1.5-flash"
GEMINI_MODEL = os.environ.get("GEMINI_MODEL") or CONFIG['gemini'].get('model', "gemini-1.5-flash")

print(f"Server Config: Model={GEMINI_MODEL}, Key={'Set' if GEMINI_API_KEY else 'Missing'}")

STEMS = CONSTANTS['STEMS']
BRANCHES = CONSTANTS['BRANCHES']
SI_HUA_TABLE = CONSTANTS['SI_HUA_TABLE']

def call_gemini_api(prompt, system_prompt="", stream=True):
    """å‘¼å« Google Gemini API çš„é€šç”¨å‡½å¼"""
    if not GEMINI_API_KEY:
        return None
    
    full_prompt = f"{system_prompt}\n\n{prompt}"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    
    headers = {'Content-Type': 'application/json'}
    payload = {
        "contents": [{"parts": [{"text": full_prompt}]}],
        "generationConfig": {
            "temperature": CONFIG['gemini'].get('temperature', 0.7),
            "maxOutputTokens": CONFIG['gemini'].get('max_output_tokens', 1024),
        },
        "safetySettings": [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
    }
    
    max_retries = 8
    for attempt in range(max_retries):
        try:
            # Generate request with 120s timeout
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            
            if response.status_code == 429:
                # Force raise to trigger retry logic
                raise requests.exceptions.HTTPError("429 Too Many Requests", response=response)
            
            response.raise_for_status()
            data = response.json()
            
            # Safety Check: If candidates missing, print raw response
            if 'candidates' not in data:
                 print(f"Gemini API - No Candidates (Safety?): {data}")
                 return None
                 
            return data['candidates'][0]['content']['parts'][0]['text']

        except requests.exceptions.HTTPError as e:
            if e.response and e.response.status_code == 429:
                # Aggressive Backoff Strategy: 5s, 10s, 15s...
                wait_time = (attempt + 1) * 5 
                print(f"Gemini API 429 (Attempt {attempt+1}/{max_retries}). Resource exhausted. Sleeping {wait_time}s...")
                time.sleep(wait_time)
                continue
            else:
                print(f"Gemini API Fail: {e}")
                return None
        except Exception as e:
            print(f"Gemini API Error: {e}")
            return None
    return None

def get_current_year_ganzhi():
    """Calculates the Heavenly Stem and Earthly Branch for the current year."""
    now = datetime.now()
    year = now.year
    if now.month < 2 or (now.month == 2 and now.day < 4):
        year -= 1
    stem_idx = (year - 4) % 10
    branch_idx = (year - 4) % 12
    return STEMS[stem_idx], BRANCHES[branch_idx]

def detect_intent_and_context(prompt, chart_data):
    instructions = ""
    injected_data = ""
    prompt_lower = prompt.lower()
    
    topics = {
        "studies": {
            "keywords": ["å­¸æ¥­", "è€ƒè©¦", "æˆç¸¾", "å‡å­¸", "å”¸æ›¸", "è®€æ›¸", "è€ƒé‹"],
            "focus": "å®˜ç¥¿å®®ã€çˆ¶æ¯å®®ã€æ–‡æ˜Œã€æ–‡æ›²ã€åŒ–ç§‘ã€é­é‰",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€å­¸æ¥­èˆ‡è€ƒé‹ã€‘ã€‚è«‹é‡é»æŸ¥çœ‹å®˜ç¥¿å®®æ°£æ•¸ï¼Œä»¥åŠæ–‡æ˜Œã€æ–‡æ›²ã€åŒ–ç§‘ç­‰æ˜Ÿæ›œçš„åˆ†å¸ƒã€‚è‹¥æœ‰å‡¶æ˜Ÿå¹²æ“¾ï¼Œè«‹çµ¦äºˆåŒ–è§£ä¹‹å»ºè­°ã€‚"
        },
        "wealth": {
            "keywords": ["è²¡é‹", "è³ºéŒ¢", "æŠ•è³‡", "è‚¡ç¥¨", "å½©åˆ¸", "è–ªæ°´", "æ”¶å…¥", "ç ´è²¡"],
            "focus": "è²¡å¸›å®®ã€ç”°å®…å®®ã€ç¦å¾·å®®ã€ç¥¿å­˜ã€åŒ–ç¥¿ã€æ­¦æ›²ã€å¤ªé™°ã€è²ªç‹¼",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€è²¡é‹èˆ‡æŠ•è³‡ã€‘ã€‚è«‹é‡é»æŸ¥çœ‹è²¡å¸›å®®å¼·å¼±ã€ç”°å®…å®®å®ˆè²¡èƒ½åŠ›ï¼Œä¸¦å°‹æ‰¾ç¥¿å­˜ã€åŒ–ç¥¿ç­‰è²¡æ˜Ÿã€‚è«‹ç›´æ–·æ­£è²¡èˆ‡åè²¡æ©Ÿé‹ã€‚"
        },
        "love": {
            "keywords": ["æ„Ÿæƒ…", "å©šå§»", "æ¡ƒèŠ±", "å¦ä¸€åŠ", "å°è±¡", "çµå©š", "é›¢å©š", "åˆ†æ‰‹"],
            "focus": "å¤«å¦»å®®ã€ç¦å¾·å®®ã€ç´…é¸ã€å¤©å–œã€è²ªç‹¼ã€å»‰è²ã€å¤ªé™½ã€å¤ªé™°",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€æ„Ÿæƒ…èˆ‡å©šå§»ã€‘ã€‚è«‹å¯Ÿçœ‹å¤«å¦»å®®ä¹‹ç©©å®šæ€§ï¼Œä»¥åŠç´…é¸ã€å¤©å–œç­‰æ¡ƒèŠ±æ˜Ÿã€‚è‹¥æœ‰åŒ–å¿Œæˆ–ç…æ˜Ÿï¼Œè«‹é»å‡ºæ„Ÿæƒ…å¯èƒ½çš„æ³¢æŠ˜ã€‚"
        },
        "career": {
            "keywords": ["äº‹æ¥­", "å·¥ä½œ", "å‡é·", "å‰µæ¥­", "è·å ´", "è€é—†", "è½‰è·", "å®˜ç¥¿"],
            "focus": "å®˜ç¥¿å®®ã€å¥´åƒ•å®®ã€ç´«å¾®ã€å¤ªé™½ã€å»‰è²ã€æ­¦æ›²ã€å¤©ç›¸",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€äº‹æ¥­èˆ‡è·å ´ç™¼å±•ã€‘ã€‚è«‹åˆ†æå®˜ç¥¿å®®æ ¼å±€ï¼Œåˆ¤æ–·é©åˆå‰µæ¥­æˆ–ä»»è·ï¼Œä¸¦æŸ¥çœ‹å¥´åƒ•å®®æœ‰æ²’æœ‰è²´äººæˆ–å°äººã€‚"
        },
        "health": {
            "keywords": ["å¥åº·", "ç–¾ç—…", "èº«é«”", "é–‹åˆ€", "æ„å¤–", "è¡€å…‰", "ç”Ÿç—…"],
            "focus": "ç–¾å„å®®ã€å‘½å®®ã€ç½ç…ã€å¤©åˆ‘ã€ç¾Šé™€ã€åŒ–å¿Œ",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€å¥åº·ç‹€æ³ã€‘ã€‚è«‹ç´°æŸ¥ç–¾å„å®®èˆ‡å‘½å®®ä¹‹ç…æ˜Ÿï¼Œç‰¹åˆ¥æ³¨æ„äº”è¡Œéæ—ºæˆ–éå¼±ä¹‹è™•ï¼Œæé†’é é˜²æ½›åœ¨ç–¾ç—…ã€‚"
        },
        "parents": {
            "keywords": ["çˆ¶æ¯", "çˆ¸çˆ¸", "åª½åª½", "é›™è¦ª", "å…­è¦ª", "é•·è¼©"],
            "focus": "çˆ¶æ¯å®®ã€å…„å¼Ÿå®®(æ¯å®®)ã€å¤ªé™½(çˆ¶)ã€å¤ªé™°(æ¯)",
            "instruction": "å°ˆæ³¨æ–¼åˆ†æç·£ä¸»çš„ã€çˆ¶æ¯è¦ªæƒ…ã€‘ã€‚è«‹æŸ¥çœ‹çˆ¶æ¯å®®èˆ‡å…„å¼Ÿå®®ï¼Œåˆ¤æ–·èˆ‡é›™è¦ªä¹‹ç·£åˆ†æ·±æ·ºèˆ‡åˆ‘å‰‹ã€‚"
        },
        "bazi": {
            "keywords": ["å…«å­—", "å­å¹³", "äº”è¡Œ", "æ—¥ä¸»", "å–œç”¨", "åç¥", "ç®—å‘½"],
            "focus": "å››æŸ±å…«å­—ã€æ—¥ä¸»å¼·å¼±ã€å–œç”¨ç¥ã€æµå¹´å¹²æ”¯ã€åç¥ç”Ÿå‰‹",
            "instruction": "è«‹å•Ÿå‹•ã€å­å¹³å…«å­—ã€‘è«–å‘½æ¨¡å¼ã€‚é‡é»åˆ†æã€Œæ—¥ä¸»å¼·å¼±ã€ã€ã€Œæ ¼å±€é«˜ä½ã€èˆ‡ã€Œå–œç”¨ç¥ã€ã€‚è«‹ä¾æ“šå››æŸ±å¹²æ”¯çš„æ²–åˆ‘åˆå®³ï¼Œè«–æ–·ç·£ä¸»çš„ä¸€ç”Ÿé‹å‹¢èµ·ä¼ã€‚è«‹å‹™å¿…çµåˆã€Œæµå¹´å¹²æ”¯ã€èˆ‡æœ¬å‘½çš„äº’å‹•ã€‚"
        }
    }
    
    found_topic = False
    for key, val in topics.items():
        if any(kw in prompt_lower for kw in val["keywords"]):
            instructions += f"\nã€é‡é»ä¸»é¡Œã€‘ï¼š{val['instruction']}\n(è«‹å¿½ç•¥èˆ‡æ­¤ä¸»é¡Œç„¡é—œçš„é›œè¨Šï¼Œé‡å°{val['focus']}é€²è¡Œæ·±åº¦è«–æ–·)\n"
            found_topic = True
            
    temporal_keywords = ["æµå¹´", "ä»Šå¹´", "é‹å‹¢", "æ˜å¹´", "202"] 
    is_temporal = any(kw in prompt_lower for kw in temporal_keywords)
    is_bazi = any(kw in prompt_lower for kw in topics["bazi"]["keywords"])

    if (is_temporal or "å¹´" in prompt or not found_topic) and not is_bazi: 
        y_stem, y_branch = get_current_year_ganzhi()
        sihua = SI_HUA_TABLE.get(y_stem, {})
        liu_nian_palace_name = "æœªçŸ¥"
        liu_nian_stars = []
        if chart_data:
            for palace in chart_data:
                if palace.get('zhi') == y_branch:
                    liu_nian_palace_name = palace.get('name', 'æµå¹´å‘½å®®')
                    liu_nian_stars = palace.get('stars', [])
                    break
        
        injected_data += f"\nã€æµå¹´å¤©æ©Ÿè³‡è¨Š (ç³»çµ±è‡ªå‹•æ¨æ¼”)ã€‘\n"
        injected_data += f"â— ç•¶ä¸‹å¹´ä»½ï¼š{y_stem}{y_branch}å¹´\n"
        injected_data += f"â— æµå¹´å‘½å®®ï¼šä½æ–¼ã€{y_branch}å®®ã€‘(æœ¬å‘½{liu_nian_palace_name})\n"
        injected_data += f"â— æµå¹´å››åŒ–ï¼š\n"
        injected_data += f"  - ç¥¿ï¼š{sihua.get('lu')} (åŒ–ç¥¿)\n"
        injected_data += f"  - æ¬Šï¼š{sihua.get('quan')} (åŒ–æ¬Š)\n"
        injected_data += f"  - ç§‘ï¼š{sihua.get('ke')} (åŒ–ç§‘)\n"
        injected_data += f"  - å¿Œï¼š{sihua.get('ji')} (åŒ–å¿Œ)\n"
        
        if is_temporal:
            instructions += "\nã€æ™‚ç©ºé‹å‹¢æŒ‡ä»¤ã€‘ï¼šç·£ä¸»è©¢å•é—œæ–¼æµå¹´æˆ–ç‰¹å®šæ™‚é–“çš„é‹å‹¢ã€‚è«‹å‹™å¿…çµåˆä¸Šè¿°ã€Œæµå¹´å‘½å®®ã€èˆ‡ã€Œæµå¹´å››åŒ–ã€é€²è¡Œæ¨æ¼”ã€‚æµå¹´å››åŒ–å°é‹å‹¢å½±éŸ¿ç”šé‰…ï¼Œè«‹ç‰¹åˆ¥è‘—å¢¨ã€‚\n"

    return instructions, injected_data

def _build_cors_preflight_response():
    resp = make_response()
    resp.headers.add("Access-Control-Allow-Origin", "*")
    resp.headers.add("Access-Control-Allow-Headers", "*")
    resp.headers.add("Access-Control-Allow-Methods", "*")
    return resp

def _corsify_actual_response(resp):
    resp.headers.add("Access-Control-Allow-Origin", "*")
    return resp

# --- Routes ---

@app.route('/')
def index():
    print("Root route accessed.")
    if os.path.exists('fate.html'):
        return send_file('fate.html')
    return "Error: fate.html not found", 404

@app.route('/<path:filename>')
def serve_static(filename):
    # Expanded allowed extensions
    if filename.lower().endswith(('.png', '.ico', '.jpg', '.jpeg', '.html', '.css', '.js', '.json', '.map')):
        if os.path.exists(filename):
            return send_file(filename)
    return "File Not Found", 404

@app.route('/api/save_record', methods=['POST', 'OPTIONS'])
def save_record():
    if request.method == 'OPTIONS': return _build_cors_preflight_response()
    data = request.json or {}
    print(f"æ¥æ”¶åˆ°å­˜æª”è«‹æ±‚: {data.get('name')}")
    record = {
        "timestamp": datetime.now().isoformat(), "name": data.get("name", "Unknown"),
        "gender": data.get("gender"), "birth_date": data.get("birth_date"),
        "birth_hour": data.get("birth_hour"), "lunar_date": data.get("lunar_date")
    }
    records_file = RECORD_FILE
    records = []
    if os.path.exists(records_file):
        try:
            with open(records_file, 'r', encoding='utf-8') as f: records = json.load(f)
        except: pass
    records.append(record)
    with open(records_file, 'w', encoding='utf-8') as f: json.dump(records, f, ensure_ascii=False, indent=2)
    return _corsify_actual_response(jsonify({"success": True}))

@app.route('/api/chat', methods=['POST', 'OPTIONS'])
def chat():
    if request.method == 'OPTIONS': return _build_cors_preflight_response()
    
    data = request.json or {}
    user_prompt = data.get('prompt', '')
    client_system_prompt = data.get('system_prompt', '')
    model = data.get('model', DEFAULT_MODEL)
    chart_data = data.get('chart_data')
    gender = data.get('gender', 'M')
    
    if not user_prompt: 
        return _corsify_actual_response(jsonify({"error": "No prompt provided"})), 400
    
    options = data.get('options', {})
    
    print(f"æ”¶åˆ° AI è«‹æ±‚: {user_prompt[:20]}...")
    
    matched_results = []
    if chart_data:
        try:
            print("æ­£åœ¨åŸ·è¡Œç´«å¾®è¦å‰‡å¼•æ“æª¢æ¸¬...")
            rule_file = "ziwei_rules.json"
            rules = []
            if os.path.exists(rule_file):
                with open(rule_file, 'r', encoding='utf-8') as f:
                    rules = json.load(f)
            
            chart = create_chart_from_dict(chart_data, gender=gender)
            matched_results = evaluate_rules(chart, rules)
            print(f"è¦å‰‡å¼•æ“å‘½ä¸­ {len(matched_results)} æ¢è¦å‰‡ã€‚")
        except Exception as e:
            print(f"è¦å‰‡å¼•æ“åŸ·è¡Œå¤±æ•—: {e}")

    full_system_prompt = f"""ä½ æ˜¯ã€ç´«å¾®å¤©æ©Ÿé“é•·ã€‘ï¼Œä¸€ä½ä¿®é“å¤šå¹´çš„å‘½ç†å®—å¸«ã€‚
    
ã€çµ•å°ä»»å‹™ã€‘ï¼šè«‹æ ¹æ“šç·£ä¸»æä¾›çš„ã€ç´«å¾®æ–—æ•¸å‘½ç›¤ã€‘èˆ‡ã€å…«å­—è³‡è¨Šã€‘ï¼Œé€²è¡Œå°ˆæ¥­çš„å‘½ç†æ‰¹è¨»ã€‚
ã€ç¦æ­¢è¡Œç‚ºã€‘ï¼š
1. ç¦æ­¢åˆ†æç·£ä¸»çš„ã€Œå¯«ä½œé¢¨æ ¼ã€æˆ–ã€Œè«–è¿°æ–¹å¼ã€ã€‚ç·£ä¸»æä¾›çš„æ–‡å­—æ˜¯ã€Œå‘½ç›¤æ•¸æ“šã€èˆ‡ã€Œç®—å‘½æŒ‡ä»¤ã€ï¼Œä¸æ˜¯æ–‡ç« ä½œå“ã€‚
2. ç¦æ­¢åå•ç·£ä¸»å•é¡Œï¼ˆå¦‚ã€Œè«‹å‘Šè¨´æˆ‘æ›´å¤š...ã€ï¼‰ã€‚å‘½ç›¤å·²åœ¨çœ¼å‰ï¼Œè«‹ç›´æ¥è«–æ–·ã€‚
3. ç¦æ­¢ä½¿ç”¨è‹±æ–‡ã€‚å¿…é ˆä½¿ç”¨ç´”æ­£çš„ã€å°ç£ç¹é«”ä¸­æ–‡ã€‘ã€‚

ã€è§’è‰²è¨­å®šã€‘ï¼š
1. **éµå£ç›´æ–·**ï¼šå‰å‡¶ç¦ç¦ç›´æ¥é»å‡ºï¼Œä¸æ¨¡ç¨œå…©å¯ã€‚
2. **å¼•ç¶“æ“šå…¸**ï¼šå¼•ç”¨ã€Šç´«å¾®å¿ƒæ³•ã€‹èˆ‡ã€Šå››æŸ±å…«å­—ã€‹å£è¨£ä½è­‰ã€‚
3. **æ…ˆæ‚²æŒ‡å¼•**ï¼šåœ¨é»å‡ºå‡¶è±¡å¾Œï¼Œå¿…é ˆçµ¦äºˆæ”¹é‹å»ºè­°ã€‚

{client_system_prompt}

ã€ç´«å¾®å¿ƒæ³•ç§˜å·ã€‘
{MASTER_BOOK}

è«‹é–‹å§‹ç‚ºç·£ä¸»æ‰¹å‘½ã€‚"""

    prompt_context = user_prompt
    try:
        intent_instructions, injected_data = detect_intent_and_context(user_prompt, chart_data)
        if injected_data:
            prompt_context += injected_data
        if intent_instructions:
            full_system_prompt += intent_instructions
    except Exception as e:
        print(f"æ„åœ–åµæ¸¬å¤±æ•—: {e}")

    def generate_unified_response():
        is_daily_query = "éŒ¦å›Š" in (user_prompt + client_system_prompt)
        is_full_report = any(kw in (user_prompt + client_system_prompt) for kw in ["è©³è©•", "å‘½è­œè©³è©•", "æ ¼å±€å ±å‘Š"])

        def call_ai_engine(target_prompt, target_system_prompt):
            if GEMINI_API_KEY:
                res = call_gemini_api(target_prompt, target_system_prompt)
                if res:
                    chunk_size = 3
                    for i in range(0, len(res), chunk_size):
                        yield res[i:i+chunk_size]
                        time.sleep(0.01)
                    return True
            
            yield "ã€ç³»çµ±è¨Šæ¯ã€‘ç„¡æ³•é€£æ¥ AI æœå‹™ (Gemini Key æœªè¨­å®šæˆ–é€£ç·šå¤±æ•—)ã€‚\n"
            return False

        if is_daily_query:
            yield "ã€å¤§å¸«æ„Ÿæ‡‰ä¸­...ã€‘æ­£åœ¨ç‚ºæ‚¨æŠ½å–ä»Šæ—¥éŒ¦å›Šï¼Œè«‹ç¨å€™...\n\n"
            short_system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šç´«å¾®æ–—æ•¸çš„æ±ºç­–å®—å¸«ã€‚ä½ çš„ä»»å‹™æ˜¯é‡å°ã€æµæ—¥å‘½å®®ã€‘çµ¦äºˆä¸€å¥ç²¾è¦éŒ¦å›Šã€‚ç¦æ­¢åˆ†ææ ¼å±€ï¼Œç¦æ­¢å»¢è©±ï¼Œå­—æ•¸100å­—å…§ï¼Œå¼·åˆ¶ç¹é«”ä¸­æ–‡ã€‚"
            summary_prompt = f"è«‹æ ¹æ“šä»¥ä¸‹æµæ—¥æ•¸æ“šçµ¦äºˆä»Šæ—¥éŒ¦å›Šï¼š\n{user_prompt}"
            if (yield from call_ai_engine(summary_prompt, short_system_prompt)): return

        is_karma_query = "å‰ä¸–ä»Šç”Ÿæ•…äº‹æ¨¡å¼" in (user_prompt + client_system_prompt) or \
                         any(kw in user_prompt for kw in ["å‰ä¸–", "å› æœ", "æ¥­åŠ›", "è¼ªè¿´"])
        if is_karma_query:
            yield "ã€ä¸‰ä¸–å› æœè§£ç¢¼å ±å‘Šã€‘\n------------------------------------------\n"
            karma_system_prompt = """ä½ æ˜¯ä¸€ä½ç²¾é€šä¸‰ä¸–å› æœçš„ç´«å¾®é€šéˆå¤§å¸«ã€‚
è«‹ç•¥éä¸–ä¿—çš„è²¡å¯Œåœ°ä½åˆ†æï¼Œå°ˆæ³¨è§£è®€å‘½ç›¤ä¸­çš„ã€ç¦å¾·å®®ã€‘(éˆé­‚å‰ä¸–)ã€ã€å‘½å®®ã€‘(ä»Šç”Ÿæ¥­åŠ›) èˆ‡ã€èº«å®®ã€‘(åŸ·è¡Œæ¨¡å¼)ã€‚
è«‹ä»¥ã€Œèªªæ•…äº‹ã€çš„æ–¹å¼ï¼Œç‚ºç·£ä¸»å‹¾å‹’å‡ºä¸€å¹…å‰ä¸–ä»Šç”Ÿçš„å› æœåœ–åƒã€‚èªæ°£éœ€ç¥ç§˜ã€‚å­—æ•¸300å­—ã€‚"""
            if (yield from call_ai_engine(f"{user_prompt}", karma_system_prompt)): return

        is_ritual_query = any(kw in (user_prompt + client_system_prompt) for kw in ["è½‰é‹", "æ”¹é‹", "å„€å¼", "ä½ˆå±€"])
        if is_ritual_query:
            yield "ã€é“å®¶è½‰é‹é–‹é‹å„€å¼ã€‘\n------------------------------------------\n"
            ritual_system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šå ªè¼¿èˆ‡é“å®¶ç§‘å„€çš„é–‹é‹å¤§å¸«ã€‚è«‹è¨­è¨ˆè½‰é‹å„€å¼ã€‚å­—æ•¸300å­—ã€‚"
            if (yield from call_ai_engine(f"{user_prompt}", ritual_system_prompt)): return
            
        is_dream_query = any(kw in (user_prompt + client_system_prompt) for kw in ["è§£å¤¢", "å¤¢å¢ƒ", "å¤¢åˆ°"])
        if is_dream_query:
            yield "ã€ç´«å¾®æ½›æ„è­˜å¤¢å¢ƒè§£ç¢¼ã€‘\n------------------------------------------\n"
            dream_system_prompt = "ä½ æ˜¯ä¸€ä½çµåˆå¿ƒç†å­¸èˆ‡ç´«å¾®æ–—æ•¸çš„å¤¢å¢ƒè§£æå¸«ã€‚è«‹è§£è®€å¤¢å¢ƒã€‚å­—æ•¸300å­—ã€‚"
            if (yield from call_ai_engine(f"{user_prompt}", dream_system_prompt)): return

        is_bazi_query = any(kw in (user_prompt + client_system_prompt) for kw in ["å…«å­—", "å­å¹³", "ç®—å‘½", "äº”è¡Œ", "æ—¥ä¸»"])
        if is_bazi_query and not is_full_report:
            import re
            yield "ã€å­å¹³å…«å­—ç²¾æ‰¹ã€‘\n------------------------------------------\n"
            clean_prompt = user_prompt
            if "ã€å…¨ç›¤æ˜Ÿç³»é…ç½®ã€‘" in clean_prompt:
                clean_prompt = re.sub(r"ã€å…¨ç›¤æ˜Ÿç³»é…ç½®ã€‘ï¼š.*?ã€å…«å­—å››æŸ±è³‡è¨Šã€‘", "ã€å…«å­—å››æŸ±è³‡è¨Šã€‘", clean_prompt, flags=re.DOTALL)
            bazi_system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šã€Šå­å¹³å…«å­—ã€‹çš„å‘½ç†å®—å¸«ã€‚ç¦æ­¢æåŠç´«å¾®æ–—æ•¸ã€‚ç´”ç²¹åˆ†æå…«å­—ã€‚å­—æ•¸400å­—ã€‚"
            if (yield from call_ai_engine(f"{clean_prompt}", bazi_system_prompt)): return
            
        is_love_query = any(kw in (user_prompt + client_system_prompt) for kw in ["æ¡ƒèŠ±", "å§»ç·£", "æ„Ÿæƒ…", "æˆ€æ„›", "è„«å–®", "æ”»ç•¥"])
        if is_love_query:
            yield "ã€ç´«å¾®æˆ€æ„›æ¡ƒèŠ±æ”»ç•¥ã€‘\n------------------------------------------\n"
            love_system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šç´«å¾®åˆå©šèˆ‡æˆ€æ„›å¿ƒç†çš„å…©æ€§å°å¸«ã€‚è«‹æä¾›æ¡ƒèŠ±æˆ€æ„›æ”»ç•¥ã€‚å­—æ•¸300å­—ã€‚"
            if (yield from call_ai_engine(f"{user_prompt}", love_system_prompt)): return

        is_finance_query = any(kw in (user_prompt + client_system_prompt) for kw in ["æŠ•è³‡", "ç†è²¡", "è²¡é‹", "è‚¡ç¥¨", "æˆ¿ç”¢"])
        if is_finance_query:
            yield "ã€ç´«å¾®è²¡é‹æŠ•è³‡ä½ˆå±€ã€‘\n------------------------------------------\n"
            finance_system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šç´«å¾®æ–—æ•¸çš„è²¡å¯Œç®¡ç†å¤§å¸«ã€‚è«‹æä¾›æŠ•è³‡ç†è²¡å»ºè­°ã€‚å­—æ•¸300å­—ã€‚"
            if (yield from call_ai_engine(f"{user_prompt}", finance_system_prompt)): return

        # --- Default / Full Report ---
        if matched_results and is_full_report:
            yield "ã€å¤©æ©Ÿé‹ç®—ä¸­...ã€‘å¤§å¸«æ­£åœ¨è©³æ‰¹æ‚¨çš„å‘½ç›¤æ ¼å±€ï¼Œè«‹ç¨å€™...\n\n"
            yield "ã€ç´«å¾®å‘½è­œæ ¼å±€é€æ¢ç²¾æ‰¹ã€‘\n"
            yield "==========================================\n"
            
            explanation_system_prompt = """ä½ æ˜¯ä¸€ä½ç²¾é€šç´«å¾®æ–—æ•¸çš„å‘½ç†å¤§å¸«ã€‚
ç¾åœ¨ï¼Œä½ æœƒæ”¶åˆ°ä¸€å€‹ç‰¹å®šçš„ã€Œå‘½ç†æ ¼å±€ã€ã€‚
è«‹ä½ é‡å°é€™å€‹æ ¼å±€é€²è¡Œã€ç™½è©±è§£é‡‹ã€‘ã€‚
å‘Šè¨´ç·£ä¸»ï¼šé€™å€‹æ ¼å±€ä»£è¡¨ä»€éº¼æ„æ€ï¼Ÿå°äººç”Ÿæœ‰ä»€éº¼å…·é«”å½±éŸ¿ï¼ˆå‰å‡¶ã€æ€§æ ¼ã€é‹å‹¢ï¼‰ï¼Ÿ
è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦é‡è¤‡é¡Œç›®ï¼Œå­—æ•¸ 50-80 å­—ã€‚"""

            group_a = [r for r in matched_results if r.get("rule_group") == "A"]
            group_b = [r for r in matched_results if r.get("rule_group") == "B"]
            group_c = [r for r in matched_results if r.get("rule_group") == "C"]
            print(f"DEBUG: Found Groups - A:{len(group_a)}, B:{len(group_b)}, C:{len(group_c)}")
            print(f"DEBUG: Gemini Key Present? {bool(GEMINI_API_KEY)}")

            def process_group(group, title):
                if not group: return
                yield f"\n{title}\n------------------------------------------\n"
                for res in group:
                    palace = res.get('detected_palace_names', 'å…¨ç›¤')
                    desc = res.get('description', '')
                    text = res.get('text', '')
                    yield f"â— ã€{palace}ã€‘{desc}ï¼š{text}\n"
                    
                    if GEMINI_API_KEY:
                        mini_prompt = f"è«‹è§£é‡‹ç´«å¾®æ–—æ•¸æ ¼å±€ï¼šã€Œ{desc}ã€ã€‚\næ ¼å±€å…§å®¹ï¼šã€Œ{text}ã€ã€‚\né€™ä»£è¡¨ä»€éº¼æ„æ€ï¼Ÿ"
                        try:
                            time.sleep(1)
                            explanation = call_gemini_api(mini_prompt, explanation_system_prompt)
                            if explanation:
                                yield f"  â†³ ğŸ’¡å¤§å¸«è§£è®€ï¼š{explanation}\n\n"
                            else:
                                yield f"  (å¤§å¸«æ²ˆé»˜...)\n\n"
                        except Exception as e:
                            yield f"  (é€£ç·šç•°å¸¸: {str(e)})\n\n"
                    else:
                        yield f"  (è©³æƒ…è«‹åƒé–±å¤ç±ï¼Œæœªè¨­å®šAPI Key)\n\n"

            yield from process_group(group_a, "ä¸€ã€ æ˜Ÿæ›œåå®ˆèˆ‡ç¥ç…ç‰¹å¾µ")
            yield from process_group(group_b, "äºŒã€ å‘½å®®å®®å¹²é£›åŒ–")
            yield from process_group(group_c, "ä¸‰ã€ å®®ä½é–“çš„äº¤äº’é£›åŒ–")
            
            yield f"{'='*40}\n"
            yield "ã€æ‰¹è¨»å®Œæˆã€‘ä»¥ä¸Šç‚ºæ‚¨çš„å‘½ç›¤æ ¼å±€é€æ¢è§£æã€‚\n"
            return

            
        rules_context_str = ""
        if matched_results:
            for r in matched_results[:25]: 
                rules_context_str += f"- {r.get('description')}ï¼š{r.get('text')}\n"
        
        summary_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ã€å‘½ç›¤åŸå§‹æ•¸æ“šã€‘èˆ‡ã€æ ¼å±€åµæ¸¬å ±å‘Šã€‘ï¼Œç‚ºç·£ä¸»é€²è¡Œæœ€çµ‚çš„ã€å‘½è­œè©³è©•èˆ‡é‹å‹¢é æ¸¬ã€‘ã€‚
ã€å‘½ç›¤åŸå§‹æ•¸æ“šã€‘ï¼š{user_prompt}
ã€åµæ¸¬æ ¼å±€ã€‘ï¼š{rules_context_str}
ã€æŒ‡ä»¤ã€‘ï¼šç¶œåˆè®ºæ–­ã€‚"""
        
        # Only call summary IF NOT full report (though currently full report returns above)
        # But this logic was "Summary ONLY" in backend_fix, here it seems mixed.
        # The user wanted to CANCEL the conclusion.
        # So providing return above effectively cancels it.
        
        if (yield from call_ai_engine(summary_prompt, full_system_prompt)): return
        
        yield "\n[ç³»çµ±è¨Šæ¯: ç„¡æ³•å–å¾— AI å›æ‡‰]\n"

    return Response(stream_with_context(generate_unified_response()), content_type='text/plain; charset=utf-8', headers={
        "Access-Control-Allow-Origin": "*",
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",
        "Connection": "keep-alive"
    })

if __name__ == '__main__':
    # Use environment variable for port or default to 5000
    port = int(os.environ.get("PORT", 5000))
    print(f"Starting Headless FatePurple Server on port {port}...")
    app.run(host='0.0.0.0', port=port, debug=CONFIG['server']['debug'])
